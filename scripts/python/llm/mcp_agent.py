importÂ asyncio
importÂ json
importÂ logging
importÂ os
importÂ shutil
fromÂ contextlibÂ importÂ AsyncExitStack
fromÂ typingÂ importÂ Any, Dict, List, Optional

importÂ httpx
fromÂ dotenvÂ importÂ load_dotenv
fromÂ openaiÂ importÂ OpenAI Â # OpenAI Python SDK
fromÂ mcpÂ importÂ ClientSession, StdioServerParameters
fromÂ mcp.client.stdioÂ importÂ stdio_client

# Configure logging
logging.basicConfig(
Â  Â  level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


# =============================
# é…ç½®åŠ è½½ç±»ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡åŠé…ç½®æ–‡ä»¶ï¼‰
# =============================
classÂ Configuration:
Â  Â Â """ç®¡ç† MCP å®¢æˆ·ç«¯çš„ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶"""

Â  Â Â defÂ __init__(self)Â ->Â None:
Â  Â  Â  Â  load_dotenv()
Â  Â  Â  Â Â # ä»ç¯å¢ƒå˜é‡ä¸­åŠ è½½ API key, base_url å’Œ model
Â  Â  Â  Â  self.api_key = os.getenv("LLM_API_KEY")
Â  Â  Â  Â  self.base_url = os.getenv("BASE_URL")
Â  Â  Â  Â  self.model = os.getenv("MODEL")
Â  Â  Â  Â Â ifÂ notÂ self.api_key:
Â  Â  Â  Â  Â  Â Â raiseÂ ValueError("âŒ æœªæ‰¾åˆ° LLM_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")

Â  Â  @staticmethod
Â  Â Â defÂ load_config(file_path: str)Â -> Dict[str, Any]:
Â  Â  Â  Â Â """
Â  Â  Â  Â  ä» JSON æ–‡ä»¶åŠ è½½æœåŠ¡å™¨é…ç½®
Â  Â  Â  Â Â 
Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  file_path: JSON é…ç½®æ–‡ä»¶è·¯å¾„
Â  Â  Â  Â Â 
Â  Â  Â  Â  Returns:
Â  Â  Â  Â  Â  Â  åŒ…å«æœåŠ¡å™¨é…ç½®çš„å­—å…¸
Â  Â  Â  Â  """
Â  Â  Â  Â Â withÂ open(file_path,Â "r")Â asÂ f:
Â  Â  Â  Â  Â  Â Â returnÂ json.load(f)


# =============================
# MCP æœåŠ¡å™¨å®¢æˆ·ç«¯ç±»
# =============================
classÂ Server:
Â  Â Â """ç®¡ç†å•ä¸ª MCP æœåŠ¡å™¨è¿æ¥å’Œå·¥å…·è°ƒç”¨"""

Â  Â Â defÂ __init__(self, name: str, config: Dict[str, Any])Â ->Â None:
Â  Â  Â  Â  self.name: str = name
Â  Â  Â  Â  self.config: Dict[str, Any] = config
Â  Â  Â  Â  self.session: Optional[ClientSession] =Â None
Â  Â  Â  Â  self.exit_stack: AsyncExitStack = AsyncExitStack()
Â  Â  Â  Â  self._cleanup_lock = asyncio.Lock()

Â  Â Â asyncÂ defÂ initialize(self)Â ->Â None:
Â  Â  Â  Â Â """åˆå§‹åŒ–ä¸ MCP æœåŠ¡å™¨çš„è¿æ¥"""
Â  Â  Â  Â Â # command å­—æ®µç›´æ¥ä»é…ç½®è·å–
Â  Â  Â  Â  command = self.config["command"]
Â  Â  Â  Â Â ifÂ commandÂ isÂ None:
Â  Â  Â  Â  Â  Â Â raiseÂ ValueError("command ä¸èƒ½ä¸ºç©º")

Â  Â  Â  Â  server_params = StdioServerParameters(
Â  Â  Â  Â  Â  Â  command=command,
Â  Â  Â  Â  Â  Â  args=self.config["args"],
Â  Â  Â  Â  Â  Â  env={**os.environ, **self.config["env"]}Â ifÂ self.config.get("env")Â elseÂ None,
Â  Â  Â  Â  )
Â  Â  Â  Â Â try:
Â  Â  Â  Â  Â  Â  stdio_transport =Â awaitÂ self.exit_stack.enter_async_context(
Â  Â  Â  Â  Â  Â  Â  Â  stdio_client(server_params)
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  read_stream, write_stream = stdio_transport
Â  Â  Â  Â  Â  Â  session =Â awaitÂ self.exit_stack.enter_async_context(
Â  Â  Â  Â  Â  Â  Â  Â  ClientSession(read_stream, write_stream)
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â awaitÂ session.initialize()
Â  Â  Â  Â  Â  Â  self.session = session
Â  Â  Â  Â Â exceptÂ ExceptionÂ asÂ e:
Â  Â  Â  Â  Â  Â  logging.error(f"Error initializing serverÂ {self.name}:Â {e}")
Â  Â  Â  Â  Â  Â Â awaitÂ self.cleanup()
Â  Â  Â  Â  Â  Â Â raise

Â  Â Â asyncÂ defÂ list_tools(self)Â -> List[Any]:
Â  Â  Â  Â Â """è·å–æœåŠ¡å™¨å¯ç”¨çš„å·¥å…·åˆ—è¡¨

Â  Â  Â  Â  Returns:
Â  Â  Â  Â  Â  Â  å·¥å…·åˆ—è¡¨
Â  Â  Â  Â  """
Â  Â  Â  Â Â ifÂ notÂ self.session:
Â  Â  Â  Â  Â  Â Â raiseÂ RuntimeError(f"ServerÂ {self.name}Â not initialized")
Â  Â  Â  Â  tools_response =Â awaitÂ self.session.list_tools()
Â  Â  Â  Â  tools = []
Â  Â  Â  Â Â forÂ itemÂ inÂ tools_response:
Â  Â  Â  Â  Â  Â Â ifÂ isinstance(item, tuple)Â andÂ item[0] ==Â "tools":
Â  Â  Â  Â  Â  Â  Â  Â Â forÂ toolÂ inÂ item[1]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tools.append(Tool(tool.name, tool.description, tool.inputSchema))
Â  Â  Â  Â Â returnÂ tools

Â  Â Â asyncÂ defÂ execute_tool(
Â  Â  Â  Â  self, tool_name: str, arguments: Dict[str, Any], retries: int =Â 2, delay: float =Â 1.0
Â  Â  )Â -> Any:
Â  Â  Â  Â Â """æ‰§è¡ŒæŒ‡å®šå·¥å…·ï¼Œå¹¶æ”¯æŒé‡è¯•æœºåˆ¶

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  tool_name: å·¥å…·åç§°
Â  Â  Â  Â  Â  Â  arguments: å·¥å…·å‚æ•°
Â  Â  Â  Â  Â  Â  retries: é‡è¯•æ¬¡æ•°
Â  Â  Â  Â  Â  Â  delay: é‡è¯•é—´éš”ç§’æ•°

Â  Â  Â  Â  Returns:
Â  Â  Â  Â  Â  Â  å·¥å…·è°ƒç”¨ç»“æœ
Â  Â  Â  Â  """
Â  Â  Â  Â Â ifÂ notÂ self.session:
Â  Â  Â  Â  Â  Â Â raiseÂ RuntimeError(f"ServerÂ {self.name}Â not initialized")
Â  Â  Â  Â  attempt =Â 0
Â  Â  Â  Â Â whileÂ attempt < retries:
Â  Â  Â  Â  Â  Â Â try:
Â  Â  Â  Â  Â  Â  Â  Â  logging.info(f"ExecutingÂ {tool_name}Â on serverÂ {self.name}...")
Â  Â  Â  Â  Â  Â  Â  Â  result =Â awaitÂ self.session.call_tool(tool_name, arguments)
Â  Â  Â  Â  Â  Â  Â  Â Â returnÂ result
Â  Â  Â  Â  Â  Â Â exceptÂ ExceptionÂ asÂ e:
Â  Â  Â  Â  Â  Â  Â  Â  attempt +=Â 1
Â  Â  Â  Â  Â  Â  Â  Â  logging.warning(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â f"Error executing tool:Â {e}. AttemptÂ {attempt}Â ofÂ {retries}."
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â ifÂ attempt < retries:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logging.info(f"Retrying inÂ {delay}Â seconds...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â awaitÂ asyncio.sleep(delay)
Â  Â  Â  Â  Â  Â  Â  Â Â else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logging.error("Max retries reached. Failing.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â raise

Â  Â Â asyncÂ defÂ cleanup(self)Â ->Â None:
Â  Â  Â  Â Â """æ¸…ç†æœåŠ¡å™¨èµ„æº"""
Â  Â  Â  Â Â asyncÂ withÂ self._cleanup_lock:
Â  Â  Â  Â  Â  Â Â try:
Â  Â  Â  Â  Â  Â  Â  Â Â awaitÂ self.exit_stack.aclose()
Â  Â  Â  Â  Â  Â  Â  Â  self.session =Â None
Â  Â  Â  Â  Â  Â Â exceptÂ ExceptionÂ asÂ e:
Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Error during cleanup of serverÂ {self.name}:Â {e}")


# =============================
# å·¥å…·å°è£…ç±»
# =============================
classÂ Tool:
Â  Â Â """å°è£… MCP è¿”å›çš„å·¥å…·ä¿¡æ¯"""

Â  Â Â defÂ __init__(self, name: str, description: str, input_schema: Dict[str, Any])Â ->Â None:
Â  Â  Â  Â  self.name: str = name
Â  Â  Â  Â  self.description: str = description
Â  Â  Â  Â  self.input_schema: Dict[str, Any] = input_schema

Â  Â Â defÂ format_for_llm(self)Â -> str:
Â  Â  Â  Â Â """ç”Ÿæˆç”¨äº LLM æç¤ºçš„å·¥å…·æè¿°"""
Â  Â  Â  Â  args_desc = []
Â  Â  Â  Â Â ifÂ "properties"Â inÂ self.input_schema:
Â  Â  Â  Â  Â  Â Â forÂ param_name, param_infoÂ inÂ self.input_schema["properties"].items():
Â  Â  Â  Â  Â  Â  Â  Â  arg_desc =Â f"-Â {param_name}:Â {param_info.get('description',Â 'No description')}"
Â  Â  Â  Â  Â  Â  Â  Â Â ifÂ param_nameÂ inÂ self.input_schema.get("required", []):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  arg_desc +=Â " (required)"
Â  Â  Â  Â  Â  Â  Â  Â  args_desc.append(arg_desc)
Â  Â  Â  Â Â returnÂ f"""
Tool:Â {self.name}
Description:Â {self.description}
Arguments:
{chr(10).join(args_desc)}
"""


# =============================
# LLM å®¢æˆ·ç«¯å°è£…ç±»ï¼ˆä½¿ç”¨ OpenAI SDKï¼‰
# =============================
classÂ LLMClient:
Â  Â Â """ä½¿ç”¨ OpenAI SDK ä¸å¤§æ¨¡å‹äº¤äº’"""

Â  Â Â defÂ __init__(self, api_key: str, base_url: Optional[str], model: str)Â ->Â None:
Â  Â  Â  Â  self.client = OpenAI(api_key=api_key, base_url=base_url)
Â  Â  Â  Â  self.model = model

Â  Â Â defÂ get_response(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None)Â -> Any:
Â  Â  Â  Â Â """
Â  Â  Â  Â  å‘é€æ¶ˆæ¯ç»™å¤§æ¨¡å‹ APIï¼Œæ”¯æŒä¼ å…¥å·¥å…·å‚æ•°ï¼ˆfunction calling æ ¼å¼ï¼‰
Â  Â  Â  Â  """
Â  Â  Â  Â  payload = {
Â  Â  Â  Â  Â  Â Â "model": self.model,
Â  Â  Â  Â  Â  Â Â "messages": messages,
Â  Â  Â  Â  Â  Â Â "tools": tools,
Â  Â  Â  Â  }
Â  Â  Â  Â Â try:
Â  Â  Â  Â  Â  Â  response = self.client.chat.completions.create(**payload)
Â  Â  Â  Â  Â  Â Â returnÂ response
Â  Â  Â  Â Â exceptÂ ExceptionÂ asÂ e:
Â  Â  Â  Â  Â  Â  logging.error(f"Error during LLM call:Â {e}")
Â  Â  Â  Â  Â  Â Â raise


# =============================
# å¤šæœåŠ¡å™¨ MCP å®¢æˆ·ç«¯ç±»ï¼ˆé›†æˆé…ç½®æ–‡ä»¶ã€å·¥å…·æ ¼å¼è½¬æ¢ä¸ OpenAI SDK è°ƒç”¨ï¼‰
# =============================
classÂ MultiServerMCPClient:
Â  Â Â defÂ __init__(self)Â ->Â None:
Â  Â  Â  Â Â """
Â  Â  Â  Â  ç®¡ç†å¤šä¸ª MCP æœåŠ¡å™¨ï¼Œå¹¶ä½¿ç”¨ OpenAI Function Calling é£æ ¼çš„æ¥å£è°ƒç”¨å¤§æ¨¡å‹
Â  Â  Â  Â  """
Â  Â  Â  Â  self.exit_stack = AsyncExitStack()
Â  Â  Â  Â  config = Configuration()
Â  Â  Â  Â  self.openai_api_key = config.api_key
Â  Â  Â  Â  self.base_url = config.base_url
Â  Â  Â  Â  self.model = config.model
Â  Â  Â  Â  self.client = LLMClient(self.openai_api_key, self.base_url, self.model)
Â  Â  Â  Â Â # (server_name -> Server å¯¹è±¡)
Â  Â  Â  Â  self.servers: Dict[str, Server] = {}
Â  Â  Â  Â Â # å„ä¸ª server çš„å·¥å…·åˆ—è¡¨
Â  Â  Â  Â  self.tools_by_server: Dict[str, List[Any]] = {}
Â  Â  Â  Â  self.all_tools: List[Dict[str, Any]] = []

Â  Â Â asyncÂ defÂ connect_to_servers(self, servers_config: Dict[str, Any])Â ->Â None:
Â  Â  Â  Â Â """
Â  Â  Â  Â  æ ¹æ®é…ç½®æ–‡ä»¶åŒæ—¶å¯åŠ¨å¤šä¸ªæœåŠ¡å™¨å¹¶è·å–å·¥å…·
Â  Â  Â  Â  servers_config çš„æ ¼å¼ä¸ºï¼š
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  "mcpServers": {
Â  Â  Â  Â  Â  Â  Â  "sqlite": { "command": "uvx", "args": [ ... ] },
Â  Â  Â  Â  Â  Â  Â  "puppeteer": { "command": "npx", "args": [ ... ] },
Â  Â  Â  Â  Â  Â  Â  ...
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  Â  Â  """
Â  Â  Â  Â  mcp_servers = servers_config.get("mcpServers", {})
Â  Â  Â  Â Â forÂ server_name, srv_configÂ inÂ mcp_servers.items():
Â  Â  Â  Â  Â  Â  server = Server(server_name, srv_config)
Â  Â  Â  Â  Â  Â Â awaitÂ server.initialize()
Â  Â  Â  Â  Â  Â  self.servers[server_name] = server
Â  Â  Â  Â  Â  Â  tools =Â awaitÂ server.list_tools()
Â  Â  Â  Â  Â  Â  self.tools_by_server[server_name] = tools

Â  Â  Â  Â  Â  Â Â forÂ toolÂ inÂ tools:
Â  Â  Â  Â  Â  Â  Â  Â Â # ç»Ÿä¸€é‡å‘½åï¼šserverName_toolName
Â  Â  Â  Â  Â  Â  Â  Â  function_name =Â f"{server_name}_{tool.name}"
Â  Â  Â  Â  Â  Â  Â  Â  self.all_tools.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â "type":Â "function",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â "function": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â "name": function_name,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â "description": tool.description,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â "input_schema": tool.input_schema
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â Â # è½¬æ¢ä¸º OpenAI Function Calling æ‰€éœ€æ ¼å¼
Â  Â  Â  Â  self.all_tools =Â awaitÂ self.transform_json(self.all_tools)

Â  Â  Â  Â  logging.info("\nâœ… å·²è¿æ¥åˆ°ä¸‹åˆ—æœåŠ¡å™¨:")
Â  Â  Â  Â Â forÂ nameÂ inÂ self.servers:
Â  Â  Â  Â  Â  Â  srv_cfg = mcp_servers[name]
Â  Â  Â  Â  Â  Â  logging.info(f" Â -Â {name}: command={srv_cfg['command']}, args={srv_cfg['args']}")
Â  Â  Â  Â  logging.info("\næ±‡æ€»çš„å·¥å…·:")
Â  Â  Â  Â Â forÂ tÂ inÂ self.all_tools:
Â  Â  Â  Â  Â  Â  logging.info(f" Â -Â {t['function']['name']}")

Â  Â Â asyncÂ defÂ transform_json(self, json_data: List[Dict[str, Any]])Â -> List[Dict[str, Any]]:
Â  Â  Â  Â Â """
Â  Â  Â  Â  å°†å·¥å…·çš„ input_schema è½¬æ¢ä¸º OpenAI æ‰€éœ€çš„ parameters æ ¼å¼ï¼Œå¹¶åˆ é™¤å¤šä½™å­—æ®µ
Â  Â  Â  Â  """
Â  Â  Â  Â  result = []
Â  Â  Â  Â Â forÂ itemÂ inÂ json_data:
Â  Â  Â  Â  Â  Â Â ifÂ notÂ isinstance(item, dict)Â orÂ "type"Â notÂ inÂ itemÂ orÂ "function"Â notÂ inÂ item:
Â  Â  Â  Â  Â  Â  Â  Â Â continue
Â  Â  Â  Â  Â  Â  old_func = item["function"]
Â  Â  Â  Â  Â  Â Â ifÂ notÂ isinstance(old_func, dict)Â orÂ "name"Â notÂ inÂ old_funcÂ orÂ "description"Â notÂ inÂ old_func:
Â  Â  Â  Â  Â  Â  Â  Â Â continue
Â  Â  Â  Â  Â  Â  new_func = {
Â  Â  Â  Â  Â  Â  Â  Â Â "name": old_func["name"],
Â  Â  Â  Â  Â  Â  Â  Â Â "description": old_func["description"],
Â  Â  Â  Â  Â  Â  Â  Â Â "parameters": {}
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â Â ifÂ "input_schema"Â inÂ old_funcÂ andÂ isinstance(old_func["input_schema"], dict):
Â  Â  Â  Â  Â  Â  Â  Â  old_schema = old_func["input_schema"]
Â  Â  Â  Â  Â  Â  Â  Â  new_func["parameters"]["type"] = old_schema.get("type",Â "object")
Â  Â  Â  Â  Â  Â  Â  Â  new_func["parameters"]["properties"] = old_schema.get("properties", {})
Â  Â  Â  Â  Â  Â  Â  Â  new_func["parameters"]["required"] = old_schema.get("required", [])
Â  Â  Â  Â  Â  Â  new_item = {
Â  Â  Â  Â  Â  Â  Â  Â Â "type": item["type"],
Â  Â  Â  Â  Â  Â  Â  Â Â "function": new_func
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  result.append(new_item)
Â  Â  Â  Â Â returnÂ result

Â  Â Â asyncÂ defÂ chat_base(self, messages: List[Dict[str, Any]])Â -> Any:
Â  Â  Â  Â Â """
Â  Â  Â  Â  ä½¿ç”¨ OpenAI æ¥å£è¿›è¡Œå¯¹è¯ï¼Œå¹¶æ”¯æŒå¤šæ¬¡å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰ã€‚
Â  Â  Â  Â  å¦‚æœè¿”å› finish_reason ä¸º "tool_calls"ï¼Œåˆ™è¿›è¡Œå·¥å…·è°ƒç”¨åå†å‘èµ·è¯·æ±‚ã€‚
Â  Â  Â  Â  """
Â  Â  Â  Â  response = self.client.get_response(messages, tools=self.all_tools)
Â  Â  Â  Â Â # å¦‚æœæ¨¡å‹è¿”å›å·¥å…·è°ƒç”¨
Â  Â  Â  Â Â ifÂ response.choices[0].finish_reason ==Â "tool_calls":
Â  Â  Â  Â  Â  Â Â whileÂ True:
Â  Â  Â  Â  Â  Â  Â  Â  messages =Â awaitÂ self.create_function_response_messages(messages, response)
Â  Â  Â  Â  Â  Â  Â  Â  response = self.client.get_response(messages, tools=self.all_tools)
Â  Â  Â  Â  Â  Â  Â  Â Â ifÂ response.choices[0].finish_reason !=Â "tool_calls":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â break
Â  Â  Â  Â Â returnÂ response

Â  Â Â asyncÂ defÂ create_function_response_messages(self, messages: List[Dict[str, Any]], response: Any)Â -> List[Dict[str, Any]]:
Â  Â  Â  Â Â """
Â  Â  Â  Â  å°†æ¨¡å‹è¿”å›çš„å·¥å…·è°ƒç”¨è§£ææ‰§è¡Œï¼Œå¹¶å°†ç»“æœè¿½åŠ åˆ°æ¶ˆæ¯é˜Ÿåˆ—ä¸­
Â  Â  Â  Â  """
Â  Â  Â  Â  function_call_messages = response.choices[0].message.tool_calls
Â  Â  Â  Â  messages.append(response.choices[0].message.model_dump())
Â  Â  Â  Â Â forÂ function_call_messageÂ inÂ function_call_messages:
Â  Â  Â  Â  Â  Â  tool_name = function_call_message.function.name
Â  Â  Â  Â  Â  Â  tool_args = json.loads(function_call_message.function.arguments)
Â  Â  Â  Â  Â  Â Â # è°ƒç”¨ MCP å·¥å…·
Â  Â  Â  Â  Â  Â  function_response =Â awaitÂ self._call_mcp_tool(tool_name, tool_args)
Â  Â  Â  Â  Â  Â  messages.append({
Â  Â  Â  Â  Â  Â  Â  Â Â "role":Â "tool",
Â  Â  Â  Â  Â  Â  Â  Â Â "content": function_response,
Â  Â  Â  Â  Â  Â  Â  Â Â "tool_call_id": function_call_message.id,
Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â Â returnÂ messages

Â  Â Â asyncÂ defÂ process_query(self, user_query: str)Â -> str:
Â  Â  Â  Â Â """
Â  Â  Â  Â  OpenAI Function Calling æµç¨‹ï¼š
Â  Â  Â  Â  Â 1. å‘é€ç”¨æˆ·æ¶ˆæ¯ + å·¥å…·ä¿¡æ¯
Â  Â  Â  Â  Â 2. è‹¥æ¨¡å‹è¿”å› finish_reason ä¸º "tool_calls"ï¼Œåˆ™è§£æå¹¶è°ƒç”¨ MCP å·¥å…·
Â  Â  Â  Â  Â 3. å°†å·¥å…·è°ƒç”¨ç»“æœè¿”å›ç»™æ¨¡å‹ï¼Œè·å¾—æœ€ç»ˆå›ç­”
Â  Â  Â  Â  """
Â  Â  Â  Â  messages = [{"role":Â "user",Â "content": user_query}]
Â  Â  Â  Â  response = self.client.get_response(messages, tools=self.all_tools)
Â  Â  Â  Â  content = response.choices[0]
Â  Â  Â  Â  logging.info(content)
Â  Â  Â  Â Â ifÂ content.finish_reason ==Â "tool_calls":
Â  Â  Â  Â  Â  Â  tool_call = content.message.tool_calls[0]
Â  Â  Â  Â  Â  Â  tool_name = tool_call.function.name
Â  Â  Â  Â  Â  Â  tool_args = json.loads(tool_call.function.arguments)
Â  Â  Â  Â  Â  Â  logging.info(f"\n[ è°ƒç”¨å·¥å…·:Â {tool_name}, å‚æ•°:Â {tool_args}Â ]\n")
Â  Â  Â  Â  Â  Â  result =Â awaitÂ self._call_mcp_tool(tool_name, tool_args)
Â  Â  Â  Â  Â  Â  messages.append(content.message.model_dump())
Â  Â  Â  Â  Â  Â  messages.append({
Â  Â  Â  Â  Â  Â  Â  Â Â "role":Â "tool",
Â  Â  Â  Â  Â  Â  Â  Â Â "content": result,
Â  Â  Â  Â  Â  Â  Â  Â Â "tool_call_id": tool_call.id,
Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  response = self.client.get_response(messages, tools=self.all_tools)
Â  Â  Â  Â  Â  Â Â returnÂ response.choices[0].message.content
Â  Â  Â  Â Â returnÂ content.message.content

Â  Â Â asyncÂ defÂ _call_mcp_tool(self, tool_full_name: str, tool_args: Dict[str, Any])Â -> str:
Â  Â  Â  Â Â """
Â  Â  Â  Â  æ ¹æ® "serverName_toolName" æ ¼å¼è°ƒç”¨ç›¸åº” MCP å·¥å…·
Â  Â  Â  Â  """
Â  Â  Â  Â  parts = tool_full_name.split("_",Â 1)
Â  Â  Â  Â Â ifÂ len(parts) !=Â 2:
Â  Â  Â  Â  Â  Â Â returnÂ f"æ— æ•ˆçš„å·¥å…·åç§°:Â {tool_full_name}"
Â  Â  Â  Â  server_name, tool_name = parts
Â  Â  Â  Â  server = self.servers.get(server_name)
Â  Â  Â  Â Â ifÂ notÂ server:
Â  Â  Â  Â  Â  Â Â returnÂ f"æ‰¾ä¸åˆ°æœåŠ¡å™¨:Â {server_name}"
Â  Â  Â  Â  resp =Â awaitÂ server.execute_tool(tool_name, tool_args)
Â  Â  Â  Â Â returnÂ resp.contentÂ ifÂ resp.contentÂ elseÂ "å·¥å…·æ‰§è¡Œæ— è¾“å‡º"

Â  Â Â asyncÂ defÂ chat_loop(self)Â ->Â None:
Â  Â  Â  Â Â """å¤šæœåŠ¡å™¨ MCP + OpenAI Function Calling å®¢æˆ·ç«¯ä¸»å¾ªç¯"""
Â  Â  Â  Â  logging.info("\nğŸ¤– å¤šæœåŠ¡å™¨ MCP + Function Calling å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡ºã€‚")
Â  Â  Â  Â  messages: List[Dict[str, Any]] = []
Â  Â  Â  Â Â whileÂ True:
Â  Â  Â  Â  Â  Â  query = input("\nä½ : ").strip()
Â  Â  Â  Â  Â  Â Â ifÂ query.lower() ==Â "quit":
Â  Â  Â  Â  Â  Â  Â  Â Â break
Â  Â  Â  Â  Â  Â Â try:
Â  Â  Â  Â  Â  Â  Â  Â  messages.append({"role":Â "user",Â "content": query})
Â  Â  Â  Â  Â  Â  Â  Â  messages = messages[-20:] Â # ä¿æŒæœ€æ–° 20 æ¡ä¸Šä¸‹æ–‡
Â  Â  Â  Â  Â  Â  Â  Â  response =Â awaitÂ self.chat_base(messages)
Â  Â  Â  Â  Â  Â  Â  Â  messages.append(response.choices[0].message.model_dump())
Â  Â  Â  Â  Â  Â  Â  Â  result = response.choices[0].message.content
Â  Â  Â  Â  Â  Â  Â  Â Â # logging.info(f"\nAI: {result}")
Â  Â  Â  Â  Â  Â  Â  Â  print(f"\nAI:Â {result}")
Â  Â  Â  Â  Â  Â Â exceptÂ ExceptionÂ asÂ e:
Â  Â  Â  Â  Â  Â  Â  Â  print(f"\nâš ï¸ Â è°ƒç”¨è¿‡ç¨‹å‡ºé”™:Â {e}")

Â  Â Â asyncÂ defÂ cleanup(self)Â ->Â None:
Â  Â  Â  Â Â """å…³é—­æ‰€æœ‰èµ„æº"""
Â  Â  Â  Â Â awaitÂ self.exit_stack.aclose()


# =============================
# ä¸»å‡½æ•°
# =============================
asyncÂ defÂ main()Â ->Â None:
Â  Â Â # ä»é…ç½®æ–‡ä»¶åŠ è½½æœåŠ¡å™¨é…ç½®
Â  Â  config = Configuration()
Â  Â  servers_config = config.load_config("servers_config.json")
Â  Â  client = MultiServerMCPClient()
Â  Â Â try:
Â  Â  Â  Â Â awaitÂ client.connect_to_servers(servers_config)
Â  Â  Â  Â Â awaitÂ client.chat_loop()
Â  Â Â finally:
Â  Â  Â  Â Â try:
Â  Â  Â  Â  Â  Â Â awaitÂ asyncio.sleep(0.1)
Â  Â  Â  Â  Â  Â Â awaitÂ client.cleanup()
Â  Â  Â  Â Â exceptÂ RuntimeErrorÂ asÂ e:
Â  Â  Â  Â  Â  Â Â # å¦‚æœæ˜¯å› ä¸ºé€€å‡º cancel scope å¯¼è‡´çš„å¼‚å¸¸ï¼Œå¯ä»¥é€‰æ‹©å¿½ç•¥
Â  Â  Â  Â  Â  Â Â ifÂ "Attempted to exit cancel scope"Â inÂ str(e):
Â  Â  Â  Â  Â  Â  Â  Â  logging.info("é€€å‡ºæ—¶æ£€æµ‹åˆ° cancel scope å¼‚å¸¸ï¼Œå·²å¿½ç•¥ã€‚")
Â  Â  Â  Â  Â  Â Â else:
Â  Â  Â  Â  Â  Â  Â  Â Â raise

ifÂ __name__ ==Â "__main__":
Â  Â  asyncio.run(main())
